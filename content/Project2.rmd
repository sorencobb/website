title: "Project 2: Approval Ratings During The Mueller Investigation"
output:
  html_document:
    df_print: paged
---

# Soren Cobb, scc2734


```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, fig.width=8)

library(tidyr)
library(tidyverse)
library(Hmisc)
library(carData)
library(ggplot2)


class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

```

## 0. Introduction:

I chose to conduct this modeling project on a dataset called SLID within the 'carData' package. SLID is an abbreviation for the Survey of Labour and Income Dynamics; this package contains data from the "1994 wave" of this survey in Ontario, Canada. The variables it contains are:

Wages - hourly wage in USD

Education - years of schooling

Age - in years

Sex - Male or Female

Language - English, French, or Other

I was drawn to this dataset because I thought it would be interesting to explore how demographic factors can be associated with measures such as pay and schooling.


## 1. MANOVA/ANOVAs/Post-hocs

We began with MANOVA testing of wages, education and age by language and subsequently conducted ANOVAs for the significant language associations. Post-hoc t-tests were also performed to determine which of the languages differed for each variable:

```{r}
SLID1<-SLID%>%na.omit()

man<-manova(cbind(wages, education, age)~language,data=SLID1)
summary(man)

summary(aov(SLID1$wages~SLID1$language))
summary(aov(SLID1$education~SLID1$language))
summary(aov(SLID1$age~SLID1$language))

pairwise.t.test(SLID1$wages,SLID1$language,p.adj="none")
pairwise.t.test(SLID1$education,SLID1$language,p.adj="none")
pairwise.t.test(SLID1$age,SLID1$language,p.adj="none")

type1errorRate<-1-(.95)^13
type1errorRate

bonferroni_adjpvalue <- 0.05/13
bonferroni_adjpvalue
```

*Write-up:*


The MANOVA testing provided a p-value of 2.2e-16 ***, meaning that some of the numeric variables show a mean difference between language groups. Upon further investigation (ANOVA for each variable by language), only the mean differences in eduacation level and age differs amongst language groups. The pairwise t-test shows that for education level, English and Fresh speakers have a significant mean difference, with a p-value of  0.0099, as well as English and Other speakers  with a p-value of 6.2e-10. As for age, French and Other speakers have a significant mean difference with a p-value of 0.00089, as well as English and Other with a p-value of 1.4e-13. In total, 1 MANOVA was performed, 3 ANOVAS (one for each variable) and 9 t-tests (pairwise for each 3 language groups by each 3 variables) for a total of 13 tests. The type 1 error rate was thus 1-(.95)^13, or 0.4866579. To correct for this very high rate of making at least one error, the bonferroni method was used to adjust the p-value to 0.05/13, or 0.003846154. With this new p-value, the mean difference in education level between French and English speakers was no longer significant, but all other previously significant associations remained.

## 2. Randomization test (standard deviations of genders)

```{r}

SLID1%>%group_by(sex)%>%dplyr::summarize(s=sd(wages))%>%dplyr::summarize(diff(s))

set.seed(348)

rand_dist<-vector()
for(i in 1:5000){
new<-data.frame(wages=sample(SLID1$wages),gender=SLID1$sex)
rand_dist[i]<-sd(new[new$gender=="Male",]$wages)-
              sd(new[new$gender=="Female",]$wages)
}
mean(rand_dist> 1.095003)*2

{hist(rand_dist,main="Comparing Null Distribution and T-Statistic",ylab="Iterations", xlab="Random Difference in sd", xlim=c(-1.1,1.1)); abline(v = 1.095003,col="red")}


```

*Write-up:*

A randomization test on the standard deviations in wages between males and females was performed. The null hypothesis states that there is not a significant difference in standard deviations, whereas the alternative hypothesis states that there is a significant difference. To test this, the difference in standard deviations was calculated as 1.095003. Then, using a random sampling of wages with gender randomly assigned, a null distribution was built. The function: mean(rand_dist> 1.095003)*2 was used to calculate a p-value that was so small that it was rounded to 0. To check this, a graph of the null distribution and t-statistic was made, which shows that a difference of 1.095003 falls well outside the range of acceptable test statistics, thus rejecting the null hypothesis and concluding that there is a significant difference in standard deviations of wages between males and females.

## 3. Linear regression model predicting wages

```{r}

library(sandwich)
library(lmtest)

SLID3<-SLID1%>%mutate(education_c=(education-mean(education)))
linreg<-lm(wages~education_c * sex, data=SLID3)
summary(linreg)
mean(SLID3$education)

ggplot(aes(x = education_c, y = wages, color = sex), data = SLID3) +geom_point()+
  stat_smooth(method = "lm", se = FALSE, fullrange = TRUE) +xlab("Years of Education (Centered by Mean)") +ylab("Hourly Wage (USD)")

# Normality
resids<-linreg$residuals
fitvals<-linreg$fitted.values
shapiro.test(resids)

# Linearity & Homoskedasticity
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')

coeftest(linreg, vcov=vcovHC(linreg))
```

*Write-UP:*

A linear regression model was built to predict the hourly wages based on the centered education variable, gender, and its interaction. The intercept estimate is 13.77828, meaning that a female (control group) with average education level, 13.33705 years, is predicted to have an hourly wage of $13.78. For every additional year of education above the mean, hourly wages is predicted to increase by 0.89047. Furthermore, males are predicted to have an increase in wages of 3.52213 per hour. However, the interaction between males and education_c has a coefficient of -0.15501, meaning that hourly wages for males only increases by 0.73546 (0.89047-0.15501) rather than 0.89047 for each additional year of education above the mean.

The linear regression model was plotted above using ggplot with points for the real data and linear lines for predicted values. The assumptions for normality, linearity, and homoskedasticity were then checked. A shapiro test concluded that the data is not normally distributed with a p-value of < 2.2e-16. A residual plot was then produced to graphically test the linearity and homoskedasticitiy assumptions. While it appears to be linear, the graph fans out to the right and upward, suggesting that the data is heteroskedastic. 

As a result, the regression results were recomputed with robust standard errors using the vcov=vcovHC function. This increased the standard errors of the education_c and sex predictors, as well as its interaction. As a result, both the t-statistic and p-values increased for these predictors. While the education_c and sex predictors remained significant, the p-value of the interaction predictor increased from 0.0419 to 0.06265, meaning that this predictor was no longer below 0.05 and was therefore not signficant. 

Lastly, the summary of the regression model provided an adjusted R-squared value of 0.1439. This means that it is estimated that 14.39% of the variance in outcomes is explained by the model.


## 4. Bootstrapping

```{r}

boot_dat<-SLID3[sample(nrow(SLID3),replace=TRUE),]

samp_distn<-replicate(5000, {
  boot_dat<-SLID3[sample(nrow(SLID3),replace=TRUE),]
  fit0<-lm(wages~education_c * sex,data=boot_dat)
  coef(fit0)
})

## Estimated SEs
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)

```

The bootstrapped standard errors of the regression model with interaction were very similar to the robust standard errors with slight increases for all except the SE for the interaction of sex and education, which was slightly lower. The decrease was very slight and thus would be unlikely to make the interaction significant. Likewise to the robust standard errors, the bootstrapped standard errors had considerable increases for all SEs of predictors when compared to the original model. 

##5. Predicting Binary Categorical Variable (Top 20% Wage Earner)

```{r}

library(ggplot2)

wages1<-SLID3$wages
quantile(wages1, c(.85))

SLID4 <- SLID3 %>% mutate(Top20percIncome=ifelse(wages>=(quantile(wages1, c(.80))),"Yes","No")) %>% mutate(age_c=age-mean(age)) %>% mutate(top=ifelse(wages>=(quantile(wages1, c(.80))),1,0))


fit00<-glm(top~sex+education_c+age_c,data=SLID4,family=binomial(link="logit"))
coeftest(fit00)

```
*Write-UP:*

For this model, I wanted to predict the top 20% wage earners using a logistic regression. I began by creating a 'top' variable which assigned a 1 for rows with a wage value in the top 20%, and a 0 for all others. A logistic regression model was then built, using sex, education_c (education variable centered), and age_c (age variable centered) as the predictors. The intercept estimate is -2.1803636, meaning that an individual in the control group - female with average education and average age - would have odds of being a top20% wage earner of e^-2.1803636 or roughly 0.113. The sexMale coefficient suggests that males have a large increase in logodds by 0.9250467, meaning that the odds of a male with average education and age being a top 20% wage earner is e^(-2.1803636+0.9250467) or roughly 0.285. The logodds estimate for education_c was 0.2907678, meaning that for every additional year of education above the mean, the odds of being a top20% wage earner increase by a factor of e^0.2907678 (a 33.745% increase). Similarly, the estimate for age_c was 0.0656031, meaning that for every increase in age by 1 year above mean age, the odds of being a top20% wage earner increase by a factor of e^0.0656031 (a 6.78% increase).

```{r}

SLID4$prob<-predict(fit00, type = "response")
table(predict=as.numeric(SLID4$prob>.5),truth=SLID4$Top20percIncome)%>%addmargins

```

*Write-UP:*

The confusion matrix was computed and displayed above, comparing the predicted top20% wage earners and the actual top 20% wage earners. These variables were used to manually calculate Accuracy, Sensitivity (TPR), Specificity (TNR), and Precision (PPV) below:

```{r}

# Sensitivity (TPR):
TPRsensitivity<-189/811
TPRsensitivity

# Specificity:(TNR)
TNRspecificity <- 3046/3176
TNRspecificity

# Accuracy:
accuracy<-(3046+189)/3987
accuracy

# Precision (PPV):
precision<-189/319
precision
```

*Write-UP:*

The density of the logodds by the binary top20% wage earner variable was plotted below using ggplot. Then, an ROC plot was graphed and AUC was calculated using the calc_auc function of the plotROC package. The results are interpreted under the plot.

```{r}

# Graphing Density of log-odds with ggplot:
SLID4$logit<-predict(fit00) 
SLID4$Top20PercentWages<-factor(SLID4$Top20percIncome,levels=c("Yes","No"))
ggplot(SLID4,aes(logit, fill=Top20PercentWages))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)

# ROC plot:

sens<-function(p,data=SLID4, y=top) mean(SLID4[SLID4$top==1,]$prob>p)
spec<-function(p,data=SLID4, y=top) mean(SLID4[SLID4$top==0,]$prob<p)
sensitivity<-sapply(seq(0,1,.01),sens,SLID4)
specificity<-sapply(seq(0,1,.01),spec,SLID4)
ROC<-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))
ROC$TPR<-sensitivity
ROC$FPR<-1-specificity
ROC%>%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1),lty=2)+ scale_x_continuous(limits = c(0,1))

# AUC calculation:
library(plotROC)
ROCplot<-ggplot(SLID4)+geom_roc(aes(d=top,m=education_c+age_c), n.cuts=0)
calc_auc(ROCplot)

```

*Write-UP:*

The above graph has an AUC of 0.7204615. This suggest that the model does a mediocre job at predicting the top20% wage earners and is far from perfect. When the model begins to show a good TPR of > .75, the FPR is already around .3 which is not desirable. 

```{r}

# 10 Fold CV:

diags<-NULL
set.seed(348)

data1<-SLID4[sample(nrow(SLID4)),] 
folds<-cut(seq(1:nrow(SLID4)),breaks=10,labels=F) 

for(i in 1:10){
  train<-data1[folds!=i,] 
  test<-data1[folds==i,] 
  truth<-test$top
  
  fit3<- glm(top~sex+education_c+age_c+language, data=train, family="binomial")
    probs<- predict(fit3, test, type="response")
      
      diags<-rbind(diags,class_diag(probs,truth)) 
}

apply(diags,2,mean)
```
*Write-UP:*

Accuracy, sensitivity, specificity, and PPV are listed above.

